# -*- coding: utf-8 -*-
"""potato_disease_classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DhmA6_FfbXQ12tXJ7FhYZWDxCYTevJ7u
"""

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import models, layers
import matplotlib.pyplot as plt
from tensorflow.keras.utils import to_categorical
import pathlib
import os

import kagglehub

# Download latest version
path = kagglehub.dataset_download("muhammadardiputra/potato-leaf-disease-dataset")

print("Path to dataset files:", path)

dataset_dir = os.path.join(path, 'Potato')
dataset_dir = pathlib.Path(dataset_dir)
dataset_dir

IMAGE_SIZE = 256
BATCH_SIZE=32

dataset = tf.keras.preprocessing.image_dataset_from_directory(
    dataset_dir,
    shuffle=True,
    image_size=(IMAGE_SIZE, IMAGE_SIZE),
    batch_size=BATCH_SIZE
)

train_dataset_dir = os.path.join(dataset_dir, 'Train')
test_dataset_dir = os.path.join(dataset_dir, 'Test')
validate_dataset_dir = os.path.join(dataset_dir, 'Valid')

train_dataset = tf.keras.preprocessing.image_dataset_from_directory(
    train_dataset_dir,
    shuffle=True,
    image_size=(IMAGE_SIZE, IMAGE_SIZE),
    batch_size=BATCH_SIZE
)

test_dataset = tf.keras.preprocessing.image_dataset_from_directory(
    test_dataset_dir,
    shuffle=True,
    image_size=(IMAGE_SIZE, IMAGE_SIZE),
    batch_size=BATCH_SIZE
)

validate_dataset = tf.keras.preprocessing.image_dataset_from_directory(
    validate_dataset_dir,
    shuffle=True,
    image_size=(IMAGE_SIZE, IMAGE_SIZE),
    batch_size=BATCH_SIZE
)

class_names = dataset.class_names
class_names

for image_batch, labels_batch in dataset.take(1):
    #the dataset_Train_dir.take(1) will givw us the first batch of images whose size si 32
    #image batch contains image array of 32 images.....and label batch contains labels of it....
    print(image_batch.shape)

    print(labels_batch.numpy())

    plt.imshow(image_batch[0].numpy().astype("uint8")) # we need to convert it to int bzz they are in float....
    plt.title(class_names[labels_batch[0]])

    #since we have shuffled=true we will get different image everytime

for image_batch, labels_batch in dataset.take(1):
    #the dataset_Train_dir.take(1) will givw us the first batch of images whose size si 32
    #image batch contains image array of 32 images.....and label batch contains labels of it....
    print(image_batch.shape)

    print(labels_batch.numpy())

    plt.figure(figsize=(10, 10))
    for i in range(12):
        ax = plt.subplot(3, 4, i + 1)
        plt.imshow(image_batch[i].numpy().astype("uint8"))
        plt.title(class_names[labels_batch[i]])
        plt.axis("off")

train_dataset = train_dataset.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)
test_dataset = test_dataset.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)
validate_dataset = validate_dataset.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)

#.cache will cache the image for future use
# .prefetch will load load the images beforehand , where buffer size is decided by the computer itseflf using tf.data.AUTOTUNE

CHANNELS=3
EPOCHS = 10

resize_and_rescale = tf.keras.Sequential([
    layers.Resizing(IMAGE_SIZE, IMAGE_SIZE, interpolation='bilinear'), # Changed this line
    layers.Rescaling(1.0/255)
])

data_augmentation = tf.keras.Sequential([
    layers.RandomFlip("horizontal_and_vertical"),
    layers.RandomRotation(0.2)
])

Input_shape = (BATCH_SIZE,IMAGE_SIZE ,IMAGE_SIZE,CHANNELS)

model  =  models.Sequential([
    resize_and_rescale,
    data_augmentation,

    layers.Conv2D(32, (3,3), activation='relu',input_shape=Input_shape),
    #input shape requre atleat 4 params and they are (BATCH_SIZE,IMAGE_SIZE ,IMAGE_SIZE,CHANNELS) i.e 32,256,256,3
    layers.MaxPooling2D((2,2)), # in the 2x2 area it will reduce the size of image by choosing the maximum valye among this 2x2 i.e 4 values
    # just watch a lec about what it really does.....

    layers.Conv2D(64, (3,3), activation='relu'),
    layers.MaxPooling2D((2,2)),

    layers.Conv2D(128, (3,3), activation='relu'),
    layers.MaxPooling2D((2,2)),

    # layers.Conv2D(256, (3,3), activation='relu'),
    # layers.MaxPooling2D((2,2)),  no need since something was 0 dk what it was

    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(64, activation='relu'),
    layers.Dense(32, activation='relu'),
    layers.Dense(8, activation='relu'),
    layers.Dense(3, activation='softmax'), # bcz there are total 3 number of classes


])

model.build(input_shape=Input_shape)

model.summary()

model.compile(
    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),
    metrics=['accuracy']
)

model.fit(
    train_dataset ,
    epochs = EPOCHS,
    batch_size = BATCH_SIZE,
    verbose = 1,
    validation_data = validate_dataset
)

