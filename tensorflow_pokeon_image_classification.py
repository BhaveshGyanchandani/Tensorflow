# -*- coding: utf-8 -*-
"""tensorflow_pokeon_image_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aOgRvzsHkrhVldOa21yQRVegTWKMHKyI
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

import cv2
import tensorflow as tf
import PIL
import os

from tensorflow import keras
from tensorflow.keras  import layers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D

from sklearn.model_selection import train_test_split

from google.colab import drive
drive.mount('/content/drive')  # This will ask for authentication

import os
data_dir = "/content/drive/My Drive/pokemon_small_size_dataset/images"
print(os.listdir(data_dir))  # List all files in the dataset

len(os.listdir(data_dir))

import pathlib
from pathlib import Path

data_dir = Path(data_dir)
# if we dont do it then we would have to use ** to go to every subdirectory
data_dir

images = list(data_dir.glob('*/*.jpg'))
len(images)

PIL.Image.open(images[40])

directories = list(data_dir.glob('*'))

len(directories) , directories[0]

class_name = [ ]
for i in directories:
  class_name.append(str(i).split('/')[-1])

len(class_name)

image_dict = { }
for i, j in zip(class_name, directories): # binds both the class_name and directories together
  image_dict[i] = list(j.glob('*.jpg')) # adds new i : j into image_dict instead of creating or refreshing a new dict everytime

len(image_dict)

image_label={ }

for i,j in zip(class_name,range(0,len(class_name))):
  image_label[i]=j

len(image_label)

img  =  cv2.imread(image_dict['Wartortle'][0])

img

#since our image dimension are so varying we want to make the dimesion of all the images same to train our model for it , to do it we will use resize

cv2.resize(img,(180,180)).shape

X, y =[] , []

for pokemon_name, images in image_dict.items():
  # here flowe_name is key and images is the by default value of it...its all by default when we go in a loop in a dictionary......

    for image in images:
        img = cv2.imread(str(image)) # open the image and convert it into numpy array cz we cant resize it firectly and store in img so we can resize
        resized_img = cv2.resize(img,(180,180)) # now reesize it
        X.append(resized_img) # append the str of this resized image in X .....like X contain the numpy array of it.....
        y.append(image_label[pokemon_name])

len(X) , len(y)

X = np.array(X)  # X is a list of images (each image is a numpy array)
y = np.array(y)

X_train , X_test , y_train , y_test = train_test_split(X,y , test_size=0.3 , random_state=42)

#normalize/scale the train and test .....so that the data in numpy array can be in b/w 0 and 1
X_train_scaled = X_train/255
X_test_scaled = X_test/255

model = Sequential([
    layers.Conv2D(16,3,padding='same',activation='relu'),
    layers.MaxPooling2D(),

    layers.Conv2D(32,3,padding='same',activation='relu'),
    layers.MaxPooling2D(),

     layers.Conv2D(64,3,padding='same',activation='relu'),
    layers.MaxPooling2D(),

    layers.Conv2D(128,3,padding='same',activation='relu'),
    layers.MaxPooling2D(),

     layers.Flatten(),
    layers.Dense(512,activation='relu'),
    layers.Dense(256,activation='relu'),
    layers.Dense(128,activation='relu'),
    layers.Dense(64,activation='relu'),
    layers.Dense(32,activation='relu'),
    layers.Dense(len(image_dict)) # 5 neurons as final output bcz there are totoal 5 flower type, so it is pretty clear that we want to clasify on basis of flower
    # we can use softmax at output as activaiton to get result from numpy array to a set of probablilites which is basically b.w 0 and 1 as probabilites can be b/w 0 and 1

])


model.compile(
    optimizer = tf.keras.optimizers.Adam(learning_rate=0.01),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['accuracy']
)

history = model.fit(X_train_scaled,y_train, validation_split=0.1, epochs=10)

